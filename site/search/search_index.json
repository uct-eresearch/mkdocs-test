{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The University of Cape Town's eResearch Centre is a distributed centre that promotes the use of advanced information technologies in supporting innovative research practice and accelerating and transforming research throughout the research lifecycle. </p>"},{"location":"data/","title":"Research data storage at UCT","text":"<p>The demands of data-intensive research represent a storage challenge. Various storage solutions available to UCT researchers include secure on-site storage, cloud storage, and online repositories. The eResearch Centre encourages the use of UCT-approved and licensed services for the storage of research data. </p> <p>Today, some areas of science are facing hundred- to thousand-fold increases in data volumes, from satellites, telescopes, microscopes, high-throughput instruments, sensor networks, accelerators and supercomputers, compared to the volumes generated only a decade ago. Other research fields also face major data management challenges. In almost every laboratory, \u2018born digital\u2019 data proliferate in files, spreadsheets or databases stored on hard drives, digital notebooks, websites, blogs and wikis. The management, curation and archiving of these digital data are becoming increasingly burdensome for researchers.</p> <p>Moreover, The Protection of Personal Information Act (POPIA) requires the reconsideration of research data storage options to ensure the protection of personal information relating to data subjects.</p>"},{"location":"data/access/","title":"Access","text":""},{"location":"data/access/#how-to-request-access-to-the-rds","title":"How to request access to the RDS?","text":"<p>The RDS is a paid service, and you can find details about costs here. Service and infrastructure costs are linked to eRA Pre-Awards to assist in budgeting for research grant applications.</p> <p>For further enquiries or to request research data storage, email us or log a request on the ICTS Service Desk.</p> <p>Documentation for users and prospective users of the RDS can be downloaded here - RDS Documentation V1.2 and Folder Structure and Permissions.   </p>"},{"location":"data/options/","title":"Options","text":""},{"location":"data/options/#research-data-store-rds","title":"Research Data Store (RDS)","text":"<p>Managed by ICTS,  this infrastructure was established to provide huge pools of research data storage at a discounted price. Secure access to your research data is provided via UCT credentials and data can be accessed from anywhere in the world. </p> <p>The data is stored on Dell MD3060 storage arrays, hosted in the highly secure Upper Campus Data Centre. A copy of the data is replicated to UCT storage array in the Neotel (Liquid Telecom) data centre in Diep River, Cape Town.</p>"},{"location":"data/options/#online-storage-in-a-repository","title":"Online storage in a repository","text":"<p>Researchers can store and publish research data in an online repository. All recommended repositories will generate a DOI (Digital Object Identifier), which is a secure and persistent identifier. This makes it easier for researchers to comply with institutional, NRF and other funder requirements. Once data is uploaded, researchers can assign licenses and control how their research outputs are accessed.</p> <p>Importantly, UCT doesn\u2019t mandate a specific repository, and depending on the discipline, there are various discipline-specific repositories to choose from. UCT\u2019s recommended online repository is ZivaHub, which we offer support and guidance for. ZivaHub (Figshare) is an excellent generalist repository and is suitable for the widest range of data types and formats.  </p> <p>Some funders may require using a specific repository, and other repositories may be more attractive because they are discipline-specific. We recommend using  www.re3data.org, an international registry of research data repositories, to search for a suitable alternative repository.</p> <p></p>"},{"location":"hpc/","title":"Welcome to the UCT HPC Documentation Site","text":"<ul> <li> <p> HPC Account Creation</p> <p>UCT researchers and students can apply for an HPC account. </p> <p> Apply now</p> </li> <li> <p> Scientific Software Installation</p> <p>Custom software applications compatible with Red Hat or Centos can be installed on the HPC.</p> <p> Request installation</p> </li> <li> <p> Data Storage and Access</p> <p>Fast IO scratch disk for data processing and options for longterm specialised research data storage are available.</p> <p> Learn more</p> </li> <li> <p> User Training</p> <p>The HPC team offers quarterly, in-person training for existing cluster users. Self-paced options are also available.</p> <p> Start here </p> </li> </ul>"},{"location":"hpc/changelog/","title":"What\u2019s New","text":"<p>Recent changes to systems, software modules, policies, and scheduled maintenance.</p>"},{"location":"hpc/data/archive/","title":"Archive &amp; Retention","text":"<p>Plan for cold storage, retrieval times, and compliance with retention rules.</p>"},{"location":"hpc/data/databases/","title":"Databases &amp; Datasets","text":"<p>Connect to database services and discover hosted/reference datasets.</p>"},{"location":"hpc/data/filesystems-quotas/","title":"Filesystems &amp; Quotas","text":""},{"location":"hpc/data/filesystems-quotas/#scratch-for-computing","title":"<code>/scratch</code> for Computing","text":"<p><code>/scratch</code> is mounted on all worker nodes. The file system is intended to provide fast disk I/O. The <code>/scratch</code> file system is shared space and is intended for temporary processing, not long term storage. Should the <code>/scratch</code> file system run out of space all users are affected.</p> <p>Users are granted a default quota of 100GB in <code>/scratch</code> and are requested to apply for additional storage by e-mailing the HPC administrators. </p>"},{"location":"hpc/data/filesystems-quotas/#what-should-you-do-on-scratch","title":"What should you do on <code>/scratch</code>?","text":"<p>This is the preferred place to put any intermediate files required while a job is executing. </p> <p>We encourage users who have large datasets to upload them directly to <code>/scratch</code>, not <code>/home</code>.</p> <p>The output of all computation should also be directed to <code>/scratch</code>.</p> <p><code>scratch</code> is not backed up</p> <p><code>/scratch</code> is purposely built as a computational work space and is not intended for long term storage.</p> <p>Any data which is lost from <code>/scratch</code> cannot be recovered.</p>"},{"location":"hpc/data/filesystems-quotas/#what-should-you-not-do-on-scratch","title":"What should you not do on `scratch?","text":"<p><code>scratch</code> should not be used for long-term storage. Users are expected to move their data from <code>/scratch</code> to long term storage as part of their workflow. </p> <p>Scripts and other data that cannot easily be replicated should not be stored in <code>/scratch</code>.</p> <p>Data that you wish to keep must be downloaded and removed from <code>/scratch</code>.</p>"},{"location":"hpc/data/filesystems-quotas/#home-for-xyz","title":"<code>/home</code> for XYZ","text":"<p>Please note that you may only store 10GB of data on <code>/home</code>.</p>"},{"location":"hpc/data/manage/","title":"Manage Your Data","text":"<p>Organize projects, track metadata, and collaborate without clobbering files.</p>"},{"location":"hpc/data/transfer/","title":"Move Data (scp/rsync/Globus)","text":"<p>Select the right tool and settings for fast, reliable data transfers.</p>"},{"location":"hpc/help/connection/","title":"Connection Problems","text":"<p>Fix SSH, VPN, and SSO problems with quick checks and logs to collect.</p>"},{"location":"hpc/help/debugging/","title":"Debugging Jobs","text":"<p>Create a minimal reproducer and use debuggers/profilers to isolate issues.</p>"},{"location":"hpc/help/faqs/","title":"Frequently Asked Questions","text":"<p>Straight answers to the top questions we receive from users.</p>"},{"location":"hpc/help/job-failures/","title":"Common Job Failures","text":"<p>Diagnose OOM, walltime, missing modules, and node errors quickly.</p>"},{"location":"hpc/help/support/","title":"Who to Contact","text":"<p>How to open a ticket with the right details and expected response times.</p>"},{"location":"hpc/help/support/#contact-us","title":"Contact Us","text":""},{"location":"hpc/perf/gpu/","title":"GPU Performance","text":"<p>Increase occupancy, manage memory movement, and avoid PCIe bottlenecks.</p>"},{"location":"hpc/perf/io/","title":"I/O Best Practices","text":"<p>Reduce small\u2011file overhead and use parallel I/O/striping effectively.</p>"},{"location":"hpc/perf/mpi-openmp/","title":"MPI/OpenMP/Hybrid","text":"<p>Bind ranks/threads correctly and pick collective and I/O strategies.</p>"},{"location":"hpc/perf/tools/","title":"Profiling &amp; Tracing Tools","text":"<p>Use site\u2011provided profilers and viewers to find bottlenecks.</p>"},{"location":"hpc/policy/allocations/","title":"Allocations","text":""},{"location":"hpc/policy/allocations/#overview","title":"Overview","text":"<p>Resources are assigned to partitions which can be thought of as queues.</p> Partition Description Nodes Cores / node Max cores / user Time limit ada : 100 series Fast cores, less RAM 100-115 48 200 250 hours ada : 200 series Slower cores, more RAM 200-226 40 200 250 hours curie Alternate partition 600-607 64 64 100 hours l40s GPU partition 012-015 48 (4 GPU cards) 96 (8 GPU cards) 48 hours gpumk Compsci GPU partition 005-008 32 (4 GPU cards) varies Private a100 GPU partition 009-011 56 (4 GPU cards) varies varies sadacc Private 128-135 44 (4 GPU cards) 176 1 hour <p>Understanding your allocations</p> <p>Researchers are assigned to an account which is analogous to a group, normally their department or research group, for instance maths, compsci etc. A researcher may also be assigned to additional accounts. Accounts may also be limited to specific partitions, hence a researcher may submit to the ada partition using their maths account, but may only submit to the GPU partition using their mathsgpu account for example.</p>"},{"location":"hpc/policy/allocations/#specifying-time-limits","title":"Specifying time limits","text":"<p>If you do not specify a time limit then your job will inherit the default maximum partition time. You may however not want this, as the shorter the wall time specified, the more likely a queued (waiting) job is to be selected by the backfill scheduler to jump the queue. The scheduler can only do this if the job guarantees it will finish in a specified, short period of time.</p> <p>Time format in SLURM</p> <p>Before starting it is important to understand the format of the time parameter to avoid ambiguity and confusion. Acceptable time formats include \u201cminutes\u201d, \u201cminutes:seconds\u201d, \u201chours:minutes:seconds\u201d, \u201cdays-hours\u201d, \u201cdays-hours:minutes\u201d and \u201cdays-hours:minutes:seconds\u201d.  This option applies to job and step allocations. If you do not specify a wall time then the partition\u2019s default wall time will be applied to your job. This will potentially disadvantage you in terms of job priority, so if you know that your job will finish in a certain time then specify that wall time in your sbatch script. When you do this the scheduler can more easily backfill your job, in other words allow it to jump the queue and start sooner.</p> <p>Some examples:</p> <ul> <li>50 = 50 minutes</li> <li>50:00 = 50 minutes</li> <li>50:00:00 = 50 hours</li> <li>2-2 = 50 hours (2 days and 2 hours)</li> <li>2-2:00 = 50 hours (2 days and 2 hours)</li> <li>2-2:00:00 = 50 hours (2 days and 2 hours)</li> </ul>"},{"location":"hpc/policy/allocations/#initial-job-node-limitations","title":"Initial job node limitations","text":"<p>Parallel jobs using MPI can address cores on multiple nodes using the nodes= directive. However if your job is not capable of running in MPI mode reserving more than one node will not make your job run faster. As some researchers do not fully understand the distinction between nodes and cores we sometimes find non-MPI jobs reserving cores on nodes that are not used. In order to avoid this waste of resource we have set the number of nodes that can be reserved for a job to 1.  In order to increase this limit please contact us.</p>"},{"location":"hpc/policy/allocations/#undergraduate-and-honours-work","title":"Undergraduate and Honours work","text":"<p>The UCT HPC cluster is intended for postgraduate supervised work that will result in a MSc, PhD or peer reviewed paper published in an accredited journal. We do however grant undergrads limited access to the cluster as we believe that this is the best time to learn how to use Linux and HPC schedulers. Undergrads are limited to 80 simultaneous cores and 100 hour jobs and honours students are limited to 120 simultaneous cores 170 hour jobs.</p>"},{"location":"hpc/policy/aup/","title":"User Policy &amp; Acceptable Use","text":"<p>By logging on to the ICTS HPC cluster you are agreeing to abide by UCT\u2019s policy and rules on Internet and email use as well as the HPC Acceptable Use Policy terms and conditions outlined below;</p>"},{"location":"hpc/policy/aup/#general-hpc-terms-and-conditions","title":"General HPC terms and conditions","text":"<ul> <li>Computing resources including but not limited to CPUs, RAM, disk space, internet bandwidth and networking are to be used for research purposes only.</li> <li>User accounts may not be shared.</li> <li>No commercial work is permitted.</li> <li>No proxy work, you may not submit jobs on behalf of someone else.</li> </ul>"},{"location":"hpc/policy/aup/#revocation-of-access","title":"Revocation of access","text":"<p>Any infringement of the above terms, or if the following abuse is noted, may result in your user account being suspended and/or disciplinary action:</p> <ul> <li>Running jobs or any high load process on the head node.</li> <li>Viewing or editing extremely large files on the head node.</li> <li>Fraudulent requests for HPC resources.</li> <li>Causing a file system to run out of space.</li> <li>Allowing illegal access to the cluster.</li> <li>Downloading non-research related files or data.</li> </ul>"},{"location":"hpc/policy/aup/#abandoned-accounts","title":"Abandoned accounts","text":"<ul> <li>Accounts that have never been logged into will be deleted after 12 months.</li> <li>Accounts not accessed in over a year will be deleted in the event we are unable to contact the researcher to establish a continued need.</li> </ul>"},{"location":"hpc/policy/aup/#core-memory-and-time-limits","title":"Core, Memory and Time limits","text":"<p>By default, users will have access to a set number of cores.  This can be boosted if the cluster is under-utilised or if the usage patterns of a researcher\u2019s jobs can be anticipated. If jobs are queued and the cluster is severely under-used then these jobs will be allowed to run by discretion of the HPC system administrators. The limits will be altered over time as hardware and usage patterns change in order to achieve maximum efficiency and usage of the cluster.</p> <p>If your jobs are known to have a low wall time and\\or you make use of the wall time directive then depending on the current state of the cluster your core usage may be increased substantially at the discretion of the administrators.</p> <p>The maximum wall and times are displayed on the dashboard under partition parameters. Additional partition attributes can be found under resource allocation.</p>"},{"location":"hpc/policy/aup/#space-usage","title":"Space usage","text":"<p><code>/home</code>:   There is limited disk space on <code>/home</code> and this is shared with other researchers. You are limited to 10GB in <code>/home</code> for scripts or reference data. This volume is protected by ICTS\u2019s backup system although there is a cost for restores.</p> <p><code>/scratch</code>:   The <code>/scratch</code> volume provides extremely high speed disk access for transient research data. You will be granted a 50GB quota by default. The <code>/scratch</code> volume is volatile and is not backed up. Users are expected to move data off this volume once computation is completed. Old files will automatically be deleted if the volume starts to run out of space. The 50GB quota can be increased on request.</p> <p><code>eresearchData</code>: Long term storage for research data.</p> Quota Lifespan (days) 50GB (default) indefinite 500GB indefinite 1TB 12 months by agreement 5TB Dependent on agreement Over 5TB Dependent on agreement <p>Understanding your quota</p> <p>Storage allocation is applied to a user on the <code>/scratch</code> file system. These are not directory quotas, they are user quotas applied to the entire file system.</p> <p>Checking your quota</p> <p>You can check your quota and usage stats with the <code>myquota</code> command. Click here to learn more.</p> <p>Please note</p> <p>Using the <code>touch</code> command to modify files in the <code>/scratch</code> volume will result in your user account being revoked.</p> <p>Long term storage</p> <p>Users requiring reliable long term central storage should contact eresearch@uct.ac.za. This storage is separate to the HPC cluster. </p>"},{"location":"hpc/policy/aup/#end-of-project","title":"End of project","text":"<p>When your account is deleted all files on <code>/home</code> and <code>/scratch</code> are deleted. It is your responsibility to move data off the HPC file systems when your research projects come to an end.</p>"},{"location":"hpc/policy/aup/#privacy","title":"Privacy","text":"<p>All user home directories are protected at the file system level. Researchers that request shared data areas must inform ICTS HPC staff as to which HPC users or groups may have access to these areas.</p>"},{"location":"hpc/policy/aup/#rogue-processes","title":"Rogue processes","text":"<p>In order to maintain the stability of the HPC clusters, the ICTS HPC administrators reserve the right to terminate any running job or process that does not fall within the agreement of good usage, or that threatens the running of other researchers\u2019 jobs.</p>"},{"location":"hpc/policy/aup/#data-and-disaster-recovery","title":"Data and disaster recovery","text":"<p>You are responsible for your data. Recovery costs due to data loss caused by user action or inaction will incur a cost. Only data that is stored on <code>/home</code> is backed up to tape. Data stored on these tapes will be kept for a maximum period of one year, thereafter the tapes will be recycled.</p>"},{"location":"hpc/policy/aup/#compilation-of-software","title":"Compilation of software","text":"<p>Compiling of software or installing libraries must not be done on the head node. This must be run as a job. If debugging or interaction is required then you must run an interactive job.</p>"},{"location":"hpc/policy/aup/#data-transfer","title":"Data transfer","text":"<p>You may transfer data via the head node, however it is preferred that large downloads of data are done via a worker node as part of a job.</p>"},{"location":"hpc/policy/aup/#citations","title":"Citations","text":"<p>Researchers are required to notify us of any submissions, publications or presentations that contains work conducted using the HPC resources. The following form of acknowledgement must also be included in your published work:</p> <p>Citation</p> <p>Computations were performed using facilities provided by the University of Cape Town\u2019s ICTS High Performance Computing team: hpc.uct.ac.za.</p> <p>doi.org/10.5281/zenodo.10021613.</p>"},{"location":"hpc/policy/aup/#off-campus-logins","title":"Off-Campus logins","text":"<p>If you need to access cluster from outside of UCT you will need to do so by making use of the VPN.</p>"},{"location":"hpc/policy/citation/","title":"Citing the Facility","text":"<p>Researchers are required to notify us of any submissions, publications or presentations that contains work conducted using the HPC resources. The following form of acknowledgement must also be included in your published work:</p> <p>Citation</p> <p>Computations were performed using facilities provided by the University of Cape Town\u2019s ICTS High Performance Computing team: hpc.uct.ac.za.</p> <p>doi.org/10.5281/zenodo.10021613.</p>"},{"location":"hpc/policy/investment/","title":"Investment Options","text":"<p>Buy\u2011in and co\u2011investment programs for priority access or hardware.</p>"},{"location":"hpc/policy/maintenance/","title":"Maintenance","text":"<p>ICTS has several data centers across campus and has a maintenance policy for each center. Additionally the HPC team has maintenance tasks that need to be carried out on the firmware, operating system and application software of the HPC nodes.</p> <p>Most maintenance slots do not require a full shut down of the data centers and your submitted jobs should continue running, however access to the cluster may be limited. If we are aware of a full data center shutdown it will be highlighted on this page, our blog and the dashboards. We will also attempt to contact our users well in advance.</p> <p> Maintenance dates for ICTS data centers</p>"},{"location":"hpc/policy/security/","title":"Security","text":"<p>Key management, MFA, secrets handling, and incident reporting.</p>"},{"location":"hpc/portals/accounts/","title":"Account/Allocations Portal","text":"<p>Reset MFA/SSH keys, check allocations, and request additional resources.</p>"},{"location":"hpc/portals/dashboards/","title":"Dashboards","text":"<p>View live cluster status, queue heatmaps, and personal usage charts.</p>"},{"location":"hpc/portals/ondemand/","title":"Open OnDemand","text":"<p>Launch shells, Jupyter, and IDEs via the web, and manage files graphically.</p>"},{"location":"hpc/ref/glossary/","title":"Glossary","text":"<code>Cluster</code> <p>A group of linked computers, known as nodes, that work together to perform complex computations, aggregating their processing power, memory, and storage to solve demanding tasks. </p> <code>Node</code> <p>An individual computer within a cluster, typically consisting of one or more sockets, each containing multiple cores. </p> <code>Central Processing Unit (CPU)</code> <p>The primary processor in a node, though the term can be ambiguous and is sometimes avoided due to potential misunderstandings. </p> <code>Core</code> <p>The smallest unit of computing within a processor, capable of executing instructions; modern processors have multiple cores. </p> <code>Thread</code> <p>A hardware thread allows a single core to execute multiple execution paths in parallel (via Hyperthreading), while software threads allow a single program to run multiple parallel tasks. </p>"},{"location":"hpc/ref/linux-cheatsheet/","title":"Linux Commands Quick Reference","text":"<p>The 50 most useful commands with one\u2011line examples.</p>"},{"location":"hpc/ref/slurm-cheatsheet/","title":"Slurm Directives Cheat Sheet","text":"<p>At\u2011a\u2011glance sbatch/srun/salloc options with short examples.</p>"},{"location":"hpc/software/catalog/","title":"Software Catalog","text":"<p>Browse installed applications, versions, and module names; request additions.</p>"},{"location":"hpc/software/compilers-libs/","title":"Compilers &amp; Math Libraries","text":"<p>Choose compilers and link math libraries (MKL/FFTW/OpenBLAS) for performance.</p>"},{"location":"hpc/software/containers/","title":"Containers (Apptainer)","text":"<p>Run OCI images with Apptainer, manage binds, and enable MPI inside containers.</p>"},{"location":"hpc/software/modules-basics/","title":"Modules (Basics)","text":"<p>Discover, load, and troubleshoot environment modules without conflicts.</p>"},{"location":"hpc/software/python/","title":"Python on HPC","text":"<p>Create reproducible environments (venv/conda/mamba) and build wheels on login nodes safely.</p>"},{"location":"hpc/software/r/","title":"R on HPC","text":"<p>Use renv/packrat, link BLAS/LAPACK, and compile native dependencies cleanly.</p>"},{"location":"hpc/start/access-accounts/","title":"Access &amp; Accounts","text":""},{"location":"hpc/start/access-accounts/#eligibility","title":"Eligibility","text":"<p>You will need to have an active University of Cape Town staff or student account. </p> <p>Third party accounts</p> <p>Third Party accounts are no longer permitted on the cluster.</p>"},{"location":"hpc/start/access-accounts/#before-applying","title":"Before applying","text":"<p>Before applying please read the UCT eResearch HPC Acceptable Use Policy (AUP).</p> <p> Read AUP now</p>"},{"location":"hpc/start/access-accounts/#your-pis-responsibility","title":"Your PI's responsibility","text":"<p>Before your account can be created your supervisor must approve the request.</p> <p>When selecting your PI from the dropdown list you must select the correct PI account.  Your PI will receive an automated email asking for their approval along with steps on how to approve the request. They must reply to this request from their UCT email account.</p> <p>Once they have responded to the approval email from their UCT email address the system will automatically forward your request to the HPC team who will then create your account.</p> <p>Streamlining the process</p> <p>We recommend contacting your PI once you have applied for access to ensure that they have received the approval email.   </p> <p>Common causes for failed applications</p> <ul> <li>If you select your PI\u2019s old student account then the application will fail</li> <li>If there is an auto-forward on your PI\u2019s email to another account the approval will fail.</li> <li>If your PI does not respond to the request within 2 weeks the application will fail.</li> </ul>"},{"location":"hpc/start/access-accounts/#application","title":"Application","text":"<p>To apply for an account you will need to login to the UCT ServiceNow portal. </p> <p> HPC Application Form</p> <p>Logging into ServiceNow</p> <p>Remember to log into the ServiceNow Portal with your UCT credentials.</p> <p>ServiceNow Portal Error</p> <p>If you receive the message \u201cYou are either not authorized or record is not valid.\u201d then please login to the ServiceNow portal by selecting the Login link at the top right on the page, and login with your UCT credentials.</p>"},{"location":"hpc/start/first-job/","title":"Running Your First Job","text":"<p>Create a shell script with paramaters similar to the one below:</p> <pre><code>#!/bin/sh\n#SBATCH --account maths\n#SBATCH --partition=ada\n#SBATCH --time=10:00:00\n#SBATCH --nodes=1 --ntasks=4\n#SBATCH --job-name=\"MyMathsJob\"\n#SBATCH --mail-user=MyEmail@uct.ac.za\n#SBATCH --mail-type=ALL\n\n/opt/exp_soft/softwareX/xyz -o /home/fred/testA15/myfile.txt\n</code></pre> SBATCH script for R jobsSBATCH script for Python jobsSBATCH script for MATLAB jobs <pre><code>#!/bin/sh\n#SBATCH --account=myaccount\n#SBATCH --partition=ada\n#SBATCH --nodes=1 --ntasks=1\n#SBATCH --time=10:00\n#SBATCH --job-name=\"MyJob\"\nmodule load software/R-4.3.3\nR CMD BATCH MyRScript.R\n</code></pre> <pre><code>#!/bin/sh\n#SBATCH --account=myaccount\n#SBATCH --partition=ada\n#SBATCH --nodes=1 --ntasks=1\n#SBATCH --time=10:00\n#SBATCH --job-name=\"MyJob\"\nmodule load python/miniconda3-py3.12\npython MyPythonScript.py\n</code></pre> <p>Note</p> <p>We make use of Miniconda to deploy Python. The version of Python is appended to the module name, in this case it is version 3.12.</p> <pre><code>#!/bin/sh\n#SBATCH --account=myaccount\n#SBATCH --partition=ada\n#SBATCH --nodes=1 --ntasks=1\n#SBATCH --time=10:00\n#SBATCH --job-name=\"MyJob\"\nmodule load software/matlab-R2024b\nmatlab -batch MyMatlabScript\n</code></pre> <p>Note</p> <p>It is essential to add the <code>-batch</code> parameter and also exclude the <code>.m</code> extension of the MATLAB script.</p>"},{"location":"hpc/start/linux-101/","title":"Basic Linux You\u2019ll Use","text":"<p>Twenty shell commands you\u2019ll actually need on day one, with short examples.</p>"},{"location":"hpc/start/login/","title":"Logging In","text":""},{"location":"hpc/start/login/#logging-in","title":"Logging In","text":"<p>Once you have been granted an account you will need to login to the head node of one of the clusters.</p> <p>Essential knowledge: Linux command line</p> <p>All the clusters run Linux and you will need to be comfortable working with the command prompt. If you have had no experience with the Linux command line interface we have training material as well as a how to guide. Alternately there are numerous web articles on how to use the Linux command prompt.</p> WindowsMacLinux <ol> <li> <p>You will need to download an ssh client such as PuTTY. Download the 64-bit x86 executable now.</p> <p></p> </li> <li> <p>Save putty.exe to your C: drive and run it.</p> </li> <li> <p>Enter the hostname of the cluster, for example <code>hpc.uct.ac.za</code>, and click <code>Open</code>.</p> <p>Cluster name</p> <p>Our cluster has two names that can be used interchangeably:</p> <ul> <li><code>hpc.uct.ac.za</code></li> <li><code>hex.uct.ac.za</code></li> </ul> <p> </p> </li> <li> <p>Type in your username the black PuTTY screen</p> </li> <li>Press enter</li> <li>Type in your password </li> <li> <p>Press enter (Note: The password characters will not appear when you type them, this is a security feature. Therefore it is important to pay attention when you are entering your password and be mindful of capitalisation).</p> <p></p> <p>Linux commands</p> <p>You can learn basic Linux commands for using the cluster in our Linux cheatsheet.</p> </li> <li> <p>You are successfully logged in if the prompt changes to <code>your_username@srvslshpc001</code>.</p> </li> </ol> <p>Most Apple MAC OS\u2019s come with commands for logging in preinstalled. </p> <ol> <li> <p>From <code>Applications</code> &gt; <code>Utilities</code> select <code>Terminal</code>.</p> <p></p> </li> <li> <p>In the Terminal application (black screen) type the following:</p> <pre><code>ssh username@hpc.uct.ac.za\n</code></pre> <p>Cluster name</p> <p>Our cluster has two names that can be used interchangeably:</p> <ul> <li><code>hpc.uct.ac.za</code></li> <li><code>hex.uct.ac.za</code></li> </ul> </li> <li> <p>Type in your username</p> </li> <li>Press enter</li> <li>Type in your password </li> <li>Press enter (Note: The password characters will not appear when you type them, this is a security feature. Therefore it is important to pay attention when you are entering your password and be mindful of capitalisation).</li> <li> <p>You are successfully logged in if the prompt changes to <code>your_username@srvslshpc001</code>.</p> <p>Linux commands</p> <p>You can learn basic Linux commands for using the cluster in our Linux cheatsheet.</p> </li> </ol> <p>Most Linux distributions come with commands for logging in preinstalled. </p> <ol> <li> <p>Open the terminal</p> <p></p> </li> <li> <p>Log in with the following command:     <pre><code>ssh username@hpc.uct.ac.za\n</code></pre></p> <p>Cluster name</p> <p>Our cluster has two names that can be used interchangeably:</p> <ul> <li><code>hpc.uct.ac.za</code></li> <li><code>hex.uct.ac.za</code></li> </ul> </li> <li> <p>Type in your username</p> </li> <li>Press enter</li> <li>Type in your password </li> <li>Press enter (Note: The password characters will not appear when you type them, this is a security feature. Therefore it is important to pay attention when you are entering your password and be mindful of capitalisation).</li> <li> <p>You are successfully logged in if the prompt changes to <code>your_username@srvslshpc001</code>.</p> <p>Linux commands</p> <p>You can learn basic Linux commands for using the cluster in our Linux cheatsheet.</p> </li> </ol>"},{"location":"hpc/start/login/#logging-out","title":"Logging Out","text":"<p>The same process for logging out of the HPC can be followed regardless of the operating system of your own computer.</p> <p>To exit from the cluster type <code>exit</code>.</p> <pre><code>exit\n</code></pre>"},{"location":"hpc/start/quick-start/","title":"Quick Start","text":""},{"location":"hpc/start/quick-start/#basic-information","title":"Basic information","text":"<p>The ICTS High Performance Cluster uses SLURM to schedule jobs. There is one head node that researchers login to in order to submit jobs. The <code>/home</code> and <code>/scratch</code> partitions on the head node are mounted on all worker nodes, regardless of series. </p> <p>The head node is called <code>hpc.uct.ac.za</code> as well as <code>hex.uct.ac.za</code>. These two names can be used interchangeably. </p>"},{"location":"hpc/start/quick-start/#how-to-get-started","title":"How to get started","text":"<ul> <li> Read through the Acceptable Usage Policy (AUP)</li> <li> Apply for an account<ul> <li> Ensure your PI or supervisor is ready to respond to the approval email </li> <li> Log into the ServiceNow Portal</li> <li> Complete your application on ServiceNow</li> </ul> </li> <li> Log into the HPC</li> <li> Transfer or mount your data source</li> <li></li> </ul>"},{"location":"hpc/systems/compute-nodes/","title":"Compute Nodes","text":"<p>Node flavors, NUMA considerations, and local storage behavior.</p>"},{"location":"hpc/systems/login-nodes/","title":"Login Nodes","text":"<p>What they\u2019re for, resource limits, and permitted activities.</p>"},{"location":"hpc/systems/overview/","title":"System Overview","text":"<p>A high\u2011level view of CPU/GPU types, interconnects, and storage tiers.</p>"},{"location":"hpc/systems/partitions/","title":"Partitions &amp; QoS","text":"<p>Pick the right partition/QoS to balance wait time and performance.</p>"},{"location":"hpc/training/courses/","title":"HPC training courses","text":""},{"location":"hpc/training/courses/#in-person-training","title":"In-person training","text":"<p>The HPC team runs two courses to assist UCT researchers with high performance computing:</p> <ul> <li>Introduction to Linux for High Performance Computing.</li> <li>Advanced Course in High Performance Computing.</li> </ul> <p>Note</p> <p>Attendance are limited to existing cluster users.</p> <p>Course notifications are sent out to cluster users during the year indicating course dates and how to reserve a space.</p>"},{"location":"hpc/training/courses/#self-study","title":"Self study","text":"<p>For self study purposes you may download the course material below:</p> <ul> <li>Introduction to Linux for HPC</li> <li>Advanced HPC</li> </ul>"},{"location":"hpc/training/tips/","title":"Tips &amp; Tricks","text":"<p>Small improvements that save time and reduce failures.</p>"},{"location":"hpc/training/tutorials/","title":"Guided Tutorials","text":"<p>Step\u2011by\u2011step exercises from first login to a multi\u2011node run.</p>"},{"location":"hpc/training/videos/","title":"Short Training Videos","text":"<p>2\u20135 minute clips that teach the most common HPC tasks.</p>"},{"location":"hpc/work/arrays-workflows/","title":"Arrays &amp; Workflows","text":"<p>Run parameter sweeps and chain jobs with simple dependencies or workflow tools.</p>"},{"location":"hpc/work/gpu/","title":"GPUs &amp; Accelerators","text":"<p>Choose GPU partitions and run CUDA/ROCm jobs with working examples.</p>"},{"location":"hpc/work/interactive/","title":"Interactive Jobs","text":"<p>An interactive job gives you command line access to a worker node. </p> <p>From the head node type:</p> <pre><code>    sintx\n</code></pre> <p>The cluster will indicate that you are starting an interactive job and your prompt will change to that of a worker node.  In addition the hostname format changes to black on white text:</p> <p></p> <p>Now any command you type is executed on that node. If you do not see the text \u201cStarting interactive job\u201d then you are still on the head node and should not run any heavy load processes.</p> <p>Unlike <code>salloc</code> your commands do not need to be prefaced with <code>srun</code> unless you are running OpenMPI code.</p> <p>Type <code>exit</code> to end the job and you will return to the head node.</p> <p>You can specify additional cluster parameters with the sintx command just as in an sbatch file:</p> <pre><code>      sintx --ntasks=20 --account=maths --partition=ada\n</code></pre> <p>Account and partition parameters are not mandatory unless you have access to more than one partition.</p> <p>In addition <code>sintx</code> automatically creates a <code>DISPLAY</code> environment variable should you wish to export a graphical display back to your workstation. You will however need to be running an Xclient on your desktop.</p>"},{"location":"hpc/work/interactive/#advanced-node-selection","title":"Advanced node selection","text":"<p>The ada partition consists of several tranches of worker nodes purchased over time.  The characteristics of these nodes are uniform within their tranches, however the 100 series are more powerful than the 200 series.  The architecture of these nodes can be viewed here.  By default your jobs will target the 100 range in the ada partition.  When this fills up jobs will spill over into the 200 series. If you want to avoid this and rather have your job queue until a faster node is available you must use the following directive:</p> <pre><code>    #SBATCH --constraint=large\n</code></pre> <p>The 200 series have a slightly better RAM\\core ratio, if you want your jobs to target these nodes then specify the following directive:</p> <pre><code>    #SBATCH --constraint=small\n</code></pre>"},{"location":"hpc/work/monitoring/","title":"Monitoring &amp; Efficiency","text":""},{"location":"hpc/work/monitoring/#hpc-dashboard","title":"HPC Dashboard","text":"<p>The HPC Dashboard offers a real-time view of the system.</p> <p> Check cluster status</p>"},{"location":"hpc/work/monitoring/#file-system-quota-check","title":"File System Quota Check","text":"<p>Users are encouraged to check their quotas by executing the following command:</p> <pre><code>myquota\n</code></pre> <p>This will produce two results, your quota in /home and /scratch for example:</p> <pre><code>The quota is per volume, not per folder. Quota values are refreshed every few minutes.\n\nVolume   Quota    Used  %Used\n/home    10GB     5GB   50%\n/scratch 100GB   76GB   76%\n</code></pre>"},{"location":"hpc/work/scheduler/","title":"Scheduler Basics","text":"<p>Understand partitions/queues, projects, priorities, and fair\u2011share scheduling.</p>"},{"location":"hpc/work/submit/","title":"Submit Batch Jobs","text":"<p>Structure a Slurm script and request CPUs, memory, time, and GPUs correctly.</p>"}]}